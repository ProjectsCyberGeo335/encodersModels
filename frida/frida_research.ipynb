{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39a7642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, T5EncoderModel\n",
    "import os\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda5e909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5EncoderModel(\n",
       "  (shared): Embedding(93651, 1536)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(93651, 1536)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 24)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ai-forever/FRIDA')\n",
    "model = T5EncoderModel.from_pretrained('ai-forever/FRIDA')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bfa3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(torch.nn.Module):          # вернём только last_hidden_state\n",
    "    def __init__(self, net): super().__init__(); self.net = net\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.net(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        return_dict=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f4b62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wrapper(\n",
       "  (net): T5EncoderModel(\n",
       "    (shared): Embedding(93651, 1536)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(93651, 1536)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 24)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "                (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "                (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "                (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "                (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wrapper(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0acd21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 86447, 44493,     2]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"hello there\"\n",
    "dims_input = tokenizer(input, return_tensors='pt')\n",
    "dims_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "837fc6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Граф: /home/retro0/cyberspace/projects/ml/research/encoders/frida/model_onnx/frida.onnx\n",
      "✓ Веса: /home/retro0/cyberspace/projects/ml/research/encoders/frida/model_onnx/frida.onnx.data\n"
     ]
    }
   ],
   "source": [
    "# --- 3. пути -----------------------------------------------------------------\n",
    "out_dir   = \"/home/retro0/cyberspace/projects/ml/research/encoders/frida/model_onnx\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "tmp_path  = os.path.join(out_dir, \"frida_tmp.onnx\")   # промежуточный\n",
    "onnx_path = os.path.join(out_dir, \"frida.onnx\")       # финальный граф\n",
    "data_name = \"frida.onnx.data\"                         # имя файла с весами\n",
    "\n",
    "# --- 4. первый экспорт (внешние данные включены, но ещё «рассыпаны») -------\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (dims_input['input_ids'], dims_input['attention_mask']), \n",
    "    tmp_path,\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"last_hidden_state\"],\n",
    "    \n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0:\"batch\",1:\"seq\"},\n",
    "        \"attention_mask\": {0:\"batch\",1:\"seq\"},\n",
    "        \"last_hidden_state\": {0:\"batch\",1:\"seq\"}},\n",
    "    use_external_data_format=True,     # >2 GB → обязателен\n",
    ")\n",
    "\n",
    "# --- 5. перечитываем и сохраняем «в один .data» ---------------------------\n",
    "proto = onnx.load(tmp_path, load_external_data=True)\n",
    "onnx.save_model(\n",
    "    proto, onnx_path,\n",
    "    save_as_external_data=True,        # включить внешний формат\n",
    "    all_tensors_to_one_file=True,      # ← КЛЮЧ: все веса в один файл\n",
    "    location=data_name,                # имя того самого .data\n",
    "    size_threshold=0                   # 0 → выносим все тензоры\n",
    ")\n",
    "\n",
    "os.remove(tmp_path)  # можно убрать временный файл\n",
    "\n",
    "print(\"✓ Граф:\", onnx_path)\n",
    "print(\"✓ Веса:\", os.path.join(out_dir, data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda91315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, входы: ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession('/encodersModels/frida/model_onnx/frida.onnx')\n",
    "print(\"OK, входы:\", [i.name for i in sess.get_inputs()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Токенизатор сохранён в: /home/retro0/cyberspace/projects/ml/research/encoders/frida/frida_tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# куда положить файлы\n",
    "tok_dir = \"/encodersModels/frida/frida_tokenizer\"\n",
    "os.makedirs(tok_dir, exist_ok=True)\n",
    "\n",
    "# ➊ загружаем один раз из Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/FRIDA\")\n",
    "\n",
    "# ➋ сохраняем весь набор файлов\n",
    "tokenizer.save_pretrained(tok_dir)\n",
    "\n",
    "print(f\"✓ Токенизатор сохранён в: {tok_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальная форма эмбеддингов: (1, 1536)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- 1. пути и токенизатор -------------------------------------------------\n",
    "model_dir  = \"/encodersModels/frida/model_onnx/model_onnx\"\n",
    "onnx_path  = f\"{model_dir}/frida.onnx\"          # рядом лежит frida.onnx.data\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"/encodersModels/frida/frida_tokenizer\")\n",
    "\n",
    "# --- 2. текст → числовые тензоры (numpy int64) -----------------------------\n",
    "texts = [\n",
    "    \"Пример русского предложения.\",\n",
    "]\n",
    "batch = tokenizer(\n",
    "    texts,\n",
    "    padding=True,            # выравниваем до макс. длины в батче\n",
    "    truncation=True,         # отрежем, если окажется > tokenizer.model_max_length\n",
    "    return_tensors=\"np\"      # сразу в NumPy, что нужно ORT\n",
    ")\n",
    "\n",
    "# batch[\"input_ids\"].dtype == int64 (обязательно!)\n",
    "# batch = {\"input_ids\": ..., \"attention_mask\": ...}\n",
    "\n",
    "# --- 3. создаём сессию ONNX Runtime ----------------------------------------\n",
    "opts = ort.SessionOptions()\n",
    "opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "# Если есть onnxruntime-gpu:\n",
    "# session = ort.InferenceSession(onnx_path, opts, providers=[\"CUDAExecutionProvider\"])\n",
    "session = ort.InferenceSession(onnx_path, opts, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# никаких дополнительных «подкачек» — оба файла читаются однократно при init\n",
    "\n",
    "# --- 4. инференс -----------------------------------------------------------\n",
    "# имена входов/выходов заданы при экспорте\n",
    "outputs = session.run(\n",
    "    [\"last_hidden_state\"],                         # список нужных выходов\n",
    "    {\n",
    "        \"input_ids\":      batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"]\n",
    "    }\n",
    ")\n",
    "hidden = outputs[0]          # numpy array, shape (B, L, H)  — float32\n",
    "\n",
    "# --- 5. mean‑pooling по непаддинговым токенам ------------------------------\n",
    "mask = batch[\"attention_mask\"].astype(np.float32)[..., None]   # (B, L, 1)\n",
    "embeddings = (hidden * mask).sum(axis=1) / mask.sum(axis=1)    # (B, H)\n",
    "\n",
    "print(\"Финальная форма эмбеддингов:\", embeddings.shape)\n",
    "# >>> (2, 768)    # 768 — размер скрытого слоя FRIDA‑энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fa3ac9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0240544 ,  0.0155571 , -0.01315651, ..., -0.02677798,\n",
       "        -0.02836104, -0.00849375]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbca7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "back_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
